{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train MLP with CIFAR10"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. import necessary packages"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. prepare dataset and dataloader"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_data = torchvision.datasets.CIFAR10(root='../data',\n",
    "                                          train=True,\n",
    "                                          transform=torchvision.transforms.ToTensor(),\n",
    "                                          download=True)\n",
    "test_data = torchvision.datasets.CIFAR10(root='../data',\n",
    "                                         train=False,\n",
    "                                         transform=torchvision.transforms.ToTensor(),\n",
    "                                         download=True)\n",
    "\n",
    "train_data_size = len(train_data)\n",
    "test_data_size = len(test_data)\n",
    "\n",
    "train_dataloader = DataLoader(train_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. design network architecture"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3*32*32, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. define models\n",
    "- `loss function`: cross entropy\n",
    "- `max epoch`: 50\n",
    "- `learning rate`: 1e-3\n",
    "- `optimizer`: adam"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_train = []\n",
    "plot_test = []\n",
    "train_loss = []\n",
    "test_loss = []\n",
    "plot_x = []\n",
    "\n",
    "model = MLP()\n",
    "# model = torch.load(\"../model/best_mlp.pth\")\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "max_epoch = 50\n",
    "lr = 1e-3\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=1e-3)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. start training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current step: 400/782, Loss: 1.9022704362869263\n",
      "Current step: 600/782, Loss: 1.8276209831237793\n",
      "epoch: 5, train_loss: 0.0281, test_loss: 0.0285, train_accuracy: 32.01%, test_accuracy: 31.67%\n",
      "best model saved!\n",
      "Current step: 0/782, Loss: 1.9261943101882935\n",
      "Current step: 200/782, Loss: 1.6582214832305908\n",
      "Current step: 400/782, Loss: 1.752622365951538\n",
      "Current step: 600/782, Loss: 1.6829006671905518\n",
      "epoch: 6, train_loss: 0.0259, test_loss: 0.0266, train_accuracy: 40.05%, test_accuracy: 39.38%\n",
      "best model saved!\n",
      "Current step: 0/782, Loss: 1.790103554725647\n",
      "Current step: 200/782, Loss: 1.466533899307251\n",
      "Current step: 400/782, Loss: 1.6450697183609009\n",
      "Current step: 600/782, Loss: 1.5248744487762451\n",
      "epoch: 7, train_loss: 0.0238, test_loss: 0.0249, train_accuracy: 45.84%, test_accuracy: 43.96%\n",
      "best model saved!\n",
      "Current step: 0/782, Loss: 1.6557466983795166\n",
      "Current step: 200/782, Loss: 1.3170733451843262\n",
      "Current step: 400/782, Loss: 1.5416054725646973\n",
      "Current step: 600/782, Loss: 1.487803339958191\n",
      "epoch: 8, train_loss: 0.0223, test_loss: 0.0238, train_accuracy: 48.91%, test_accuracy: 46.06%\n",
      "best model saved!\n",
      "Current step: 0/782, Loss: 1.5324946641921997\n",
      "Current step: 200/782, Loss: 1.1985708475112915\n",
      "Current step: 400/782, Loss: 1.458121418952942\n",
      "Current step: 600/782, Loss: 1.4580779075622559\n",
      "epoch: 9, train_loss: 0.0212, test_loss: 0.0232, train_accuracy: 51.35%, test_accuracy: 47.47%\n",
      "best model saved!\n",
      "Current step: 0/782, Loss: 1.521145224571228\n",
      "Current step: 200/782, Loss: 1.1579989194869995\n",
      "Current step: 400/782, Loss: 1.3928656578063965\n",
      "Current step: 600/782, Loss: 1.415132999420166\n",
      "epoch: 10, train_loss: 0.0202, test_loss: 0.0228, train_accuracy: 53.58%, test_accuracy: 48.25%\n",
      "best model saved!\n",
      "Current step: 0/782, Loss: 1.4355957508087158\n",
      "Current step: 200/782, Loss: 1.076589584350586\n",
      "Current step: 400/782, Loss: 1.3214179277420044\n",
      "Current step: 600/782, Loss: 1.3938138484954834\n",
      "epoch: 11, train_loss: 0.0194, test_loss: 0.0226, train_accuracy: 55.40%, test_accuracy: 49.32%\n",
      "best model saved!\n",
      "Current step: 0/782, Loss: 1.3931878805160522\n",
      "Current step: 200/782, Loss: 0.986285388469696\n",
      "Current step: 400/782, Loss: 1.236445426940918\n",
      "Current step: 600/782, Loss: 1.327112078666687\n",
      "epoch: 12, train_loss: 0.0187, test_loss: 0.0225, train_accuracy: 56.77%, test_accuracy: 49.39%\n",
      "best model saved!\n",
      "Current step: 0/782, Loss: 1.3460099697113037\n",
      "Current step: 200/782, Loss: 0.9277997016906738\n",
      "Current step: 400/782, Loss: 1.1634392738342285\n",
      "Current step: 600/782, Loss: 1.2688608169555664\n",
      "epoch: 13, train_loss: 0.0181, test_loss: 0.0225, train_accuracy: 58.65%, test_accuracy: 49.78%\n",
      "best model saved!\n",
      "Current step: 0/782, Loss: 1.2474052906036377\n",
      "Current step: 200/782, Loss: 0.8719416260719299\n",
      "Current step: 400/782, Loss: 1.0705028772354126\n",
      "Current step: 600/782, Loss: 1.2550934553146362\n",
      "epoch: 14, train_loss: 0.0175, test_loss: 0.0226, train_accuracy: 59.92%, test_accuracy: 49.95%\n",
      "Current step: 0/782, Loss: 1.2267887592315674\n",
      "Current step: 200/782, Loss: 0.8099233508110046\n",
      "Current step: 400/782, Loss: 1.0363134145736694\n",
      "Current step: 600/782, Loss: 1.1542366743087769\n",
      "epoch: 15, train_loss: 0.0170, test_loss: 0.0228, train_accuracy: 61.25%, test_accuracy: 49.96%\n",
      "Current step: 0/782, Loss: 1.1915948390960693\n",
      "Current step: 200/782, Loss: 0.7878289818763733\n",
      "Current step: 400/782, Loss: 0.9451258778572083\n",
      "Current step: 600/782, Loss: 1.1526927947998047\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_18348\\1817320632.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     15\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m         \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 17\u001B[1;33m         \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     18\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     19\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mi\u001B[0m \u001B[1;33m%\u001B[0m \u001B[1;36m200\u001B[0m \u001B[1;33m==\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\10341\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\optim\\optimizer.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     86\u001B[0m                 \u001B[0mprofile_name\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"Optimizer.step#{}.step\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     87\u001B[0m                 \u001B[1;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprofiler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrecord_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprofile_name\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 88\u001B[1;33m                     \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     89\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     90\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\10341\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001B[0m in \u001B[0;36mdecorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     26\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m             \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 28\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     29\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mcast\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mF\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\10341\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\optim\\sgd.py\u001B[0m in \u001B[0;36mstep\u001B[1;34m(self, closure)\u001B[0m\n\u001B[0;32m    141\u001B[0m                   \u001B[0mlr\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mlr\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    142\u001B[0m                   \u001B[0mdampening\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdampening\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 143\u001B[1;33m                   nesterov=nesterov)\n\u001B[0m\u001B[0;32m    144\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    145\u001B[0m             \u001B[1;31m# update momentum_buffers in state\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mc:\\users\\10341\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\torch\\optim\\_functional.py\u001B[0m in \u001B[0;36msgd\u001B[1;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov)\u001B[0m\n\u001B[0;32m    162\u001B[0m         \u001B[0md_p\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0md_p_list\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mi\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    163\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mweight_decay\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 164\u001B[1;33m             \u001B[0md_p\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0md_p\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0madd\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mparam\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0malpha\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mweight_decay\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    165\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    166\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mmomentum\u001B[0m \u001B[1;33m!=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "min_loss = 100000\n",
    "for epoch in range(1, 1+max_epoch):\n",
    "    total_train_loss = 0\n",
    "    for i, batch in enumerate(train_dataloader):\n",
    "        imgs, targets = batch\n",
    "        outputs = model(imgs)\n",
    "\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 200 == 0:\n",
    "            print(\"Current step: {}/{}, Loss: {}\".format(i, len(train_dataloader), loss.item()))\n",
    "\n",
    "    total_train_loss = 0\n",
    "    total_test_loss = 0\n",
    "    total_train_accuracy = 0\n",
    "    total_test_accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in train_dataloader:\n",
    "            imgs, targets = batch\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            total_train_loss += loss.item()\n",
    "            total_train_accuracy += (outputs.argmax(1) == targets).sum()\n",
    "        plot_x.append(epoch)\n",
    "        train_loss.append(total_train_loss / train_data_size)\n",
    "        plot_train.append(total_train_accuracy / train_data_size * 100)\n",
    "        for batch in test_dataloader:\n",
    "            imgs, targets = batch\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            total_test_loss += loss.item()\n",
    "            total_test_accuracy += (outputs.argmax(1) == targets).sum()\n",
    "        test_loss.append(total_test_loss / test_data_size)\n",
    "        plot_test.append(total_test_accuracy / test_data_size * 100)\n",
    "    print(\"epoch: %d, train_loss: %.4f, test_loss: %.4f, train_accuracy: %.2f%%, test_accuracy: %.2f%%\"\n",
    "          % (epoch,\n",
    "             total_train_loss / train_data_size,\n",
    "             total_test_loss / test_data_size,\n",
    "             total_train_accuracy / train_data_size * 100,\n",
    "             total_test_accuracy / test_data_size * 100))\n",
    "\n",
    "\n",
    "    if total_test_loss < min_loss:\n",
    "        min_loss = total_test_loss\n",
    "        torch.save(model, \"../model/best_mlp.pth\")\n",
    "        print(\"best model saved!\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. visualize curves"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig1 = plt.subplot(2,1,1)\n",
    "fig2 = plt.subplot(2,1,2)\n",
    "print(plot_x, plot_train)\n",
    "fig1.plot(plot_x, plot_train,  c='red', label='training data accuracy')\n",
    "fig1.plot(plot_x, plot_test, c='blue', label='test data accuracy')\n",
    "fig1.legend()\n",
    "fig2.plot(plot_x, train_loss, c='green', label='train data loss')\n",
    "fig2.plot(plot_x, test_loss, c='yellow', label='test data loss')\n",
    "fig2.legend()\n",
    "plt.savefig(\"curve.jpg\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}